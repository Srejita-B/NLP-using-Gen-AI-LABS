{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0edaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SREJITA\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c0dfa",
   "metadata": {},
   "source": [
    "# Word and sentence tokenization and filterng sentences by stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46ab3d",
   "metadata": {},
   "source": [
    "Sentence 1 : 20 years ago, we all were found in different ways in different places but all at the same moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d0dc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['20', 'years', 'ago', ',', 'we', 'all', 'were', 'found', 'in', 'different', 'ways', 'in', 'different', 'places', 'but', 'all', 'at', 'the', 'same', 'moment']\n",
      "Sentence tokens are ['20 years ago, we all were found in different ways in different places but all at the same moment']\n"
     ]
    }
   ],
   "source": [
    "s1=\"20 years ago, we all were found in different ways in different places but all at the same moment\"\n",
    "word_tokens1=word_tokenize(s1)\n",
    "sentence_tokens1=sent_tokenize(s1)\n",
    "print(\"Word tokens are\",word_tokens1)\n",
    "print(\"Sentence tokens are\",sentence_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2aba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words1= set(stopwords.words('english'))#set takes only unique value\n",
    "stop_words1=list(stop_words1)\n",
    "stop_words2=[\" \",\",\",\"!\",\"?\",\".\",\"I\",\"We\",\"me\"]\n",
    "stop_words=stop_words1+stop_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fa3dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered sentence ['20', 'years', 'ago', 'found', 'different', 'ways', 'different', 'places', 'moment']\n",
      "Length of original sentence 20\n",
      "No of words stopped 11\n",
      "Length of stop words 9\n"
     ]
    }
   ],
   "source": [
    "filter_sentence=[w for w in word_tokens1 if not w in stop_words]\n",
    "print(\"filtered sentence\",filter_sentence)\n",
    "print(\"Length of original sentence\",len(word_tokens1))\n",
    "print(\"No of words stopped\",len(word_tokens1)-len(filter_sentence))\n",
    "print(\"Length of stop words\",len(filter_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff16b87",
   "metadata": {},
   "source": [
    "Sentence 2: That our lives would be changed forever.The world was loud with carnage and sirensand then with missing voices that would never be heard again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b99ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['That', 'our', 'lives', 'would', 'be', 'changed', 'forever.The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirensand', 'then', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again']\n",
      "Sentence tokens are ['That our lives would be changed forever.The world was loud with carnage and sirensand then with missing voices that would never be heard again']\n"
     ]
    }
   ],
   "source": [
    "s1=\"That our lives would be changed forever.The world was loud with carnage and sirensand then with missing voices that would never be heard again\"\n",
    "word_tokens1=word_tokenize(s1)\n",
    "sentence_tokens1=sent_tokenize(s1)\n",
    "print(\"Word tokens are\",word_tokens1)\n",
    "print(\"Sentence tokens are\",sentence_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7436f50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered sentence ['That', 'lives', 'would', 'changed', 'forever.The', 'world', 'loud', 'carnage', 'sirensand', 'missing', 'voices', 'would', 'never', 'heard']\n",
      "Length of original sentence 24\n",
      "No of words stopped 10\n",
      "Length of new sentence 14\n"
     ]
    }
   ],
   "source": [
    "filter_sentence=[w for w in word_tokens1 if not w in stop_words]\n",
    "print(\"filtered sentence\",filter_sentence)\n",
    "print(\"Length of original sentence\",len(word_tokens1))\n",
    "print(\"No of words stopped\",len(word_tokens1)-len(filter_sentence))\n",
    "print(\"Length of new sentence\",len(filter_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0cce7",
   "metadata": {},
   "source": [
    "Sentence 3: These lives reman precious to our country and infinitely precious to many of you.Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well.For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ccf700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['These', 'lives', 'remain', 'precious', 'to', 'our', 'country', 'and', 'infinitely', 'precious', 'to', 'many', 'of', 'you.Today', ',', 'we', 'remember', 'your', 'loss', ',', 'we', 'share', 'your', 'sorrow', ',', 'and', 'we', 'honor', 'the', 'men', 'and', 'women', 'you', 'have', 'loved', 'so', 'long', 'and', 'so', 'well.For', 'those', 'too', 'young', 'to', 'recall', 'that', 'clear', 'September', 'day', ',', 'it', 'is', 'hard', 'to', 'describe', 'the', 'mix', 'of', 'feelings', 'we', 'experienced']\n",
      "Sentence tokens are ['These lives remain precious to our country and infinitely precious to many of you.Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well.For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced']\n"
     ]
    }
   ],
   "source": [
    "s1=\"These lives remain precious to our country and infinitely precious to many of you.Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well.For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced\"\n",
    "word_tokens1=word_tokenize(s1)\n",
    "sentence_tokens1=sent_tokenize(s1)\n",
    "print(\"Word tokens are\",word_tokens1)\n",
    "print(\"Sentence tokens are\",sentence_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "993a07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered sentence ['These', 'lives', 'remain', 'precious', 'country', 'infinitely', 'precious', 'many', 'you.Today', 'remember', 'loss', 'share', 'sorrow', 'honor', 'men', 'women', 'loved', 'long', 'well.For', 'young', 'recall', 'clear', 'September', 'day', 'hard', 'describe', 'mix', 'feelings', 'experienced']\n",
      "Length of original sentence 61\n",
      "No of words stopped 32\n",
      "Length of new sentence 29\n"
     ]
    }
   ],
   "source": [
    "filter_sentence=[w for w in word_tokens1 if not w in stop_words]\n",
    "print(\"filtered sentence\",filter_sentence)\n",
    "print(\"Length of original sentence\",len(word_tokens1))\n",
    "print(\"No of words stopped\",len(word_tokens1)-len(filter_sentence))\n",
    "print(\"Length of new sentence\",len(filter_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffbedf",
   "metadata": {},
   "source": [
    "Sentence 4: There was horror at the scale of destruction and awe at the bravery and kindness that rose to meet it.There was,shock!The audacity of evil and gratitude , for the heroism and decency that opposed it? In the sacrifice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66eaf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['There', 'was', 'horror', 'at', 'the', 'scale', 'of', 'destruction', 'and', 'awe', 'at', 'the', 'bravery', 'and', 'kindness', 'that', 'rose', 'to', 'meet', 'it.There', 'was', ',', 'shock', '!', 'The', 'audacity', 'of', 'evil', 'and', 'gratitude', ',', 'for', 'the', 'heroism', 'and', 'decency', 'that', 'opposed', 'it', '?', 'In', 'the', 'sacrifice']\n",
      "Sentence tokens are ['There was horror at the scale of destruction and awe at the bravery and kindness that rose to meet it.There was,shock!The audacity of evil and gratitude , for the heroism and decency that opposed it?', 'In the sacrifice']\n"
     ]
    }
   ],
   "source": [
    "s1=\"There was horror at the scale of destruction and awe at the bravery and kindness that rose to meet it.There was,shock!The audacity of evil and gratitude , for the heroism and decency that opposed it? In the sacrifice\"\n",
    "word_tokens1=word_tokenize(s1)\n",
    "sentence_tokens1=sent_tokenize(s1)\n",
    "print(\"Word tokens are\",word_tokens1)\n",
    "print(\"Sentence tokens are\",sentence_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec777991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered sentence ['There', 'horror', 'scale', 'destruction', 'awe', 'bravery', 'kindness', 'rose', 'meet', 'it.There', 'shock', 'The', 'audacity', 'evil', 'gratitude', 'heroism', 'decency', 'opposed', 'In', 'sacrifice']\n",
      "Length of original sentence 43\n",
      "No of words stopped 23\n",
      "Length of new sentence 20\n"
     ]
    }
   ],
   "source": [
    "filter_sentence=[w for w in word_tokens1 if not w in stop_words]\n",
    "print(\"filtered sentence\",filter_sentence)\n",
    "print(\"Length of original sentence\",len(word_tokens1))\n",
    "print(\"No of words stopped\",len(word_tokens1)-len(filter_sentence))\n",
    "print(\"Length of new sentence\",len(filter_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e02104",
   "metadata": {},
   "source": [
    "# Write a Python script that takes a paragraph and performs word tokenization,POS tagging,stemming,lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37755498",
   "metadata": {},
   "source": [
    "Q1 : Mary jumps in a field and following her Sam also jumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eea68c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['Mary', 'jumps', 'in', 'a', 'field', 'and', 'following', 'her', 'Sam', 'also', 'jumped']\n"
     ]
    }
   ],
   "source": [
    "text=\"Mary jumps in a field and following her Sam also jumped\"\n",
    "word_tokens=word_tokenize(text)\n",
    "print(\"Word tokens are\",word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "873d045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos tags are [('Mary', 'NNP'), ('jumps', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('field', 'NN'), ('and', 'CC'), ('following', 'VBG'), ('her', 'PRP$'), ('Sam', 'NNP'), ('also', 'RB'), ('jumped', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tags=pos_tag(word_tokens)\n",
    "print(\"Pos tags are\",pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7569881a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mari\n",
      "jump\n",
      "in\n",
      "a\n",
      "field\n",
      "and\n",
      "follow\n",
      "her\n",
      "sam\n",
      "also\n",
      "jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer   #a form of dimension reduction\n",
    "ps=PorterStemmer()\n",
    "for i in word_tokens:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef850178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " 'jump',\n",
       " 'in',\n",
       " 'a',\n",
       " 'field',\n",
       " 'and',\n",
       " 'following',\n",
       " 'her',\n",
       " 'Sam',\n",
       " 'also',\n",
       " 'jumped']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "lemmatized_words=[lem.lemmatize(word_tokens) for word_tokens in word_tokens]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f37bc2",
   "metadata": {},
   "source": [
    "That our lives would be changed forever.The world was loud with carnage and sirensand then with missing voices that would never be heard again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c4cfe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['That', 'our', 'lives', 'would', 'be', 'changed', 'forever.The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirensand', 'then', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again']\n"
     ]
    }
   ],
   "source": [
    "text1=\"That our lives would be changed forever.The world was loud with carnage and sirensand then with missing voices that would never be heard again\"\n",
    "word_tokens=word_tokenize(text1)\n",
    "print(\"Word tokens are\",word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3255b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos tags are [('That', 'DT'), ('our', 'PRP$'), ('lives', 'NNS'), ('would', 'MD'), ('be', 'VB'), ('changed', 'VBN'), ('forever.The', 'JJ'), ('world', 'NN'), ('was', 'VBD'), ('loud', 'JJ'), ('with', 'IN'), ('carnage', 'NN'), ('and', 'CC'), ('sirensand', 'NN'), ('then', 'RB'), ('with', 'IN'), ('missing', 'VBG'), ('voices', 'NNS'), ('that', 'WDT'), ('would', 'MD'), ('never', 'RB'), ('be', 'VB'), ('heard', 'RB'), ('again', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags=pos_tag(word_tokens)\n",
    "print(\"Pos tags are\",pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8335c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that\n",
      "our\n",
      "live\n",
      "would\n",
      "be\n",
      "chang\n",
      "forever.th\n",
      "world\n",
      "wa\n",
      "loud\n",
      "with\n",
      "carnag\n",
      "and\n",
      "sirensand\n",
      "then\n",
      "with\n",
      "miss\n",
      "voic\n",
      "that\n",
      "would\n",
      "never\n",
      "be\n",
      "heard\n",
      "again\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer   #a form of dimension reduction\n",
    "ps=PorterStemmer()\n",
    "for i in word_tokens:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4ba1494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That',\n",
       " 'our',\n",
       " 'life',\n",
       " 'would',\n",
       " 'be',\n",
       " 'changed',\n",
       " 'forever.The',\n",
       " 'world',\n",
       " 'wa',\n",
       " 'loud',\n",
       " 'with',\n",
       " 'carnage',\n",
       " 'and',\n",
       " 'sirensand',\n",
       " 'then',\n",
       " 'with',\n",
       " 'missing',\n",
       " 'voice',\n",
       " 'that',\n",
       " 'would',\n",
       " 'never',\n",
       " 'be',\n",
       " 'heard',\n",
       " 'again']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "lemmatized_words=[lem.lemmatize(word_tokens) for word_tokens in word_tokens]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28108a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens are ['That', 'our', 'lives', 'would', 'be', 'changed', 'forever.The', 'world', 'was', 'loud', 'with', 'carnage', 'and', 'sirensand', 'then', 'with', 'missing', 'voices', 'that', 'would', 'never', 'be', 'heard', 'again']\n"
     ]
    }
   ],
   "source": [
    "text2=\"These lives reman precious to our country and infinitely precious to many of you.Today, we remember your loss, we share your sorrow, and we honor the men and women you have loved so long and so well.For those too young to recall that clear September day, it is hard to describe the mix of feelings we experienced\"\n",
    "word_tokens=word_tokenize(text1)\n",
    "print(\"Word tokens are\",word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81ceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
